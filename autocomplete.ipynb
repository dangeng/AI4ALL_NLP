{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autocomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwtYAiNotFeC"
      },
      "source": [
        "# Autocomplete\n",
        "\n",
        "In this project we will explore different ways to create an autocomplete algorithm: similar to the ones you might find on a phone when you type. Given a partial sentence we will try to predict the word that comes next. It turns out this seemingly simple task is an incredibly important problem in modern natural language processing (NLP), and much effort has gone in to solving it.\n",
        "\n",
        "The way we'll be solving this problem is by using something called a _**bigram model**_. Don't worry about what this is for now; just know that we will have to do the following steps:\n",
        "\n",
        "- Download the data (in this project, we'll be using \"The Complete Works of Shakespeare\")\n",
        "- Clean the data (the text we get is messy, so we have to tidy it up a bit)\n",
        "- Train our model\n",
        "- Predict words and sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8hi2tR2RNH"
      },
      "source": [
        "# Setting Up\n",
        "\n",
        "Before we begin we should import some Python packages that will be useful later on. Run the following block of code to import these libraries, and take a look at the comments to see what each import does:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH9o16Cm2Y-9"
      },
      "source": [
        "import numpy as np                      # Vector and matrix math library (makes math a lot easier)\n",
        "import random                           # Functions for random operations\n",
        "import re                               # Regex (makes cleaning text a lot easier)\n",
        "from collections import Counter         # Counter utility \n",
        "from tqdm import tqdm                   # Progress bar (makes life nicer)\n",
        "import matplotlib.pyplot as plt         # Library to plot and visualize data\n",
        "from collections import defaultdict     # A special dictionary implementation\n",
        "import json                             # Library to parse JSON files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqN8se_2yLA5"
      },
      "source": [
        "# The Data\n",
        "\n",
        "As with all machine learning algorithms, before we begin we first need to find data to train with. In this project we will start off by using the \"Complete Works of Shakespeare\" which, as its name suggests, contains the totality of Shakespeare's writings. \n",
        "\n",
        "In NLP people call a dataset of text a _**corpus**_. So we will call our dataset the \"Shakespeare corpus.\"\n",
        "\n",
        "Run the following block to download the corpus and load it into python. If at any time you want to completely restart, just run this block of code again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toN6eaMxtMvZ"
      },
      "source": [
        "# Download the shakespeare corpus to `shakespeare.txt`\n",
        "!wget -O shakespeare.txt https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
        "#!sed -i '40000,$d' shakespeare.txt\n",
        "\n",
        "# Read the dataset into python\n",
        "with open('./shakespeare.txt', 'r+') as f:\n",
        "  lines = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmvKI6Nu59ty"
      },
      "source": [
        "# Exploring and Cleaning the Data\n",
        "\n",
        "Data from the internet is rarely neat and tidy. Oftentimes there will be extraneous information in our data that we don't want. Other times the data will be formatted oddly, so we'll want to standardize everything. In this section we'll take a look at our data and then _**clean**_ the corpus accordingly.\n",
        "\n",
        "First, try exploring the dataset by answering the following questions (the first one is done for you):\n",
        "\n",
        "(You can find the full dataset [here](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt) in a form that is a bit more readable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Cu9AdJ4n1M"
      },
      "source": [
        "# The dataset is in the variable `lines`. It is a list of strings, one\n",
        "# for each line in the dataset. What does the first line say?\n",
        "print(lines[0])\n",
        "\n",
        "\n",
        "\n",
        "# How many lines are in the dataset?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# How many characters are in the dataset?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# What do the first 10 lines of the corpus say?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl7ecyb4RT8X"
      },
      "source": [
        "Taking a look at your answer to the last question, it should be obvious that the first couple lines of the dataset don't actually contain any Shakespeare! In fact, the beginning and end of this dataset is filled with copyright information, which we don't want. \n",
        "\n",
        "We would much rather have a dataset of _only_ Shakespeare's writings, so we should _clean_ these pieces of text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZHkRQ9ZRNo4"
      },
      "source": [
        "# At which line does the actual Shakespeare start? (hint: it's somewhere around line 250)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# The last couple lines is also copyright text\n",
        "# At which line does the Shakespeare end? \n",
        "# (hint: it's somewhere around line 124440, or -20)\n",
        "\n",
        "### YOUR CODE HERE ##\n",
        "\n",
        "print(### YOUR ANSWER HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9RMRTPpT-lB"
      },
      "source": [
        "# Write some code to remove the copyright info \n",
        "# from the beginning and end of the dataset\n",
        "lines = ### YOUR CODE HERE ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MoTUR5NSqKy"
      },
      "source": [
        "Actually, there's more copyright text sprinkled all over the dataset. We would like to remove these lines as well. Below we've given a list of lines that we don't want in the `blacklist` variable. Write some code to remove any lines from our dataset that matches a line in the blacklist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff-y3d0oOxI1"
      },
      "source": [
        "blacklist = ['<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\\n',\n",
        "             'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\\n',\n",
        "             'PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\\n',\n",
        "             'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\\n',\n",
        "             'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\\n',\n",
        "             'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\n',\n",
        "             'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\n',\n",
        "             'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\n']\n",
        "\n",
        "\n",
        "# Remove lines from `lines` if they appear in the black list\n",
        "\n",
        "### YOUR CODE HERE ### \n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch this line!)\n",
        "assert len(lines) == 122455, \"The new `lines` list should have 122455 lines in it!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef41gJJ8_Z-S"
      },
      "source": [
        "Great, we've now removed most of the copyright text that we don't care about! \n",
        "\n",
        "Another problem is that our corpus is just a list of strings right now. What we really want is a list of _sentences_. Try doing this by answering the questions in the blocks below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ5mUhX-A0WL"
      },
      "source": [
        "# Turn the corpus from a list of strings into one big string\n",
        "# Hint: the `join` function might be helpful (but not necessary) (try googling it!)\n",
        "text = ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch this line!)\n",
        "assert len(text) == 5461111, \"`text` should have 5461111 characters in it if processed correctly!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qha553PhguLT"
      },
      "source": [
        "# Lowercase the string (hint: try googling \"lowercase string python\")\n",
        "text = ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch this line!)\n",
        "assert text.islower(), \"`text` is not completely lower case!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgkiRy1RUp0o"
      },
      "source": [
        "Now we want to remove anything that is not a letter or a punctuation mark (one of '.', '!', or '?'), any additional whitespace, and then split the string into sentences. We've written this code for you, so just read the next block and run run it.\n",
        "\n",
        "(The code below uses the python library `re`, which takes advantage of _**regular expressions**_. Regular expressions give a really powerful way to search through text. You can learn more about them [here](https://regexone.com/).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMuIiPhnUSnT"
      },
      "source": [
        "# Remove all non-alphabetical letters and non-punctuation\n",
        "pattern = re.compile('[^a-zA-Z_ .!?]+')\n",
        "text = pattern.sub('', text)\n",
        "\n",
        "# Remove extra whitespace\n",
        "text = re.sub(\"\\s+\",\" \", text)\n",
        "\n",
        "# Get sentences by splitting the string on any\n",
        "# punctuation mark ('.', '!' or '?')\n",
        "sentences = re.split('[.!?]', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBpNztGRh30b"
      },
      "source": [
        "At this point we've done everything we need to clean the data! The sentences should be in a variable called `sentences` now. Of course, the data is still a bit messy, but nobody's perfect! \n",
        "\n",
        "You should explore the sentences we now have by playing around with the block below. And then try to answer the questions below in the block afterwards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp4fpL1AhK2r"
      },
      "source": [
        "# Finds \"to be or not to be\" in the corpus, and outputs the containing sentence\n",
        "# Feel free to play with this block of code!\n",
        "\n",
        "for idx, sentence in enumerate(sentences):\n",
        "  if 'to be or not to be' in sentence:\n",
        "    target_idx = idx\n",
        "print(sentences[target_idx])\n",
        "print('Index: ' + str(target_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNy2VKURXlzU"
      },
      "source": [
        "# Can you find a line from \"Romeo and Juliet\"?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# How many sentences do we have total?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# What is the longest sentence and how long is it?\n",
        "# Can you understand what the sentence is and why it's so long?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP9gtuQajPKu"
      },
      "source": [
        "# The Bigram Model\n",
        "\n",
        "The Bigram Model sounds fancy, but it's actually pretty straightforward. Say we want to know what word should come after the word `\"computer\"`. We just look at our dataset, find all instances of the word `\"computer\"`, look at the word that comes _after_ `\"computer\"`, and tally up those words. Then we randomly choose a word based on how frequently it appears in our tally.\n",
        "\n",
        "For example, let's say we've looked at our dataset and the following words appear after `\"computer\"` (with the frequency in parentheses):\n",
        "\n",
        "- `\"software\"` (4)\n",
        "- `\"chip\"` (2)\n",
        "- `\"virus\"` (3)\n",
        "- `\"desk\"` (1)\n",
        "- `\"bananas\"` (0)\n",
        "\n",
        "Then given the question \"what word comes after `\"computer\"`?\" We would answer with the word `\"software\"` with a 40% probability, the word `\"chip\"` with a 20% probability, and so on. We should answer with the word `\"bananas\"` 0% of the time, because the word `\"bananas\"` _never_ comes after the word `\"computer\"` in our dataset.\n",
        "\n",
        "So in order to train this model, we need to build a dictionary that when given a word, gives us the tallies of the next word in our dataset.\n",
        "\n",
        "## What _is_ a Bigram???\n",
        "\n",
        "A _**bigram**_ is just a fancy way term for \"two words in a row\". For example, the sentence:\n",
        "\n",
        "`\"to be or not to be that is the question\"`\n",
        "\n",
        "contains the bigrams `\"to be\"`, `\"be or\"`, `\"or not\"`, `\"not to\"`, `\"to be\"`, `\"be that\"`, `\"that is\"`, `\"is the\"`, and `\"the question\"`. In a bigram model, we're essentially tallying up the number of bigrams in our dataset, and using these tallies to predict the next word.\n",
        "\n",
        "Google provides a service that lets you plot out the frequency of various bigrams and n-grams (`n` words in a row) in books over time. You can try it out [here](https://books.google.com/ngrams/graph?content=artificial+intelligence&year_start=1800&year_end=2019&corpus=26&smoothing=3&direct_url=t1%3B%2Cartificial%20intelligence%3B%2Cc0).\n",
        "\n",
        "## Word Counts\n",
        "\n",
        "To start, let's count all the words in our dataset first. To do this we will use the Python string function `split` and the `Counter` utility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCQ_IfBLhuCr"
      },
      "source": [
        "# Split the corpus into a list of consecutive words (hint: Googling `split` function is useful here)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "words = ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# Use the Counter class to count the words in the corpus\n",
        "# (hint: the Python documentation could be helpful here: \"https://docs.python.org/3/library/collections.html#collections.Counter\")\n",
        "\n",
        "word_freq = ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch thes lines!)\n",
        "assert len(words) == 882602, \"`words` must be a list with 882602 elements!\"\n",
        "assert type(word_freq) is Counter, \"`word_freq` must be a `Counter` object!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccT4YzPuthwH"
      },
      "source": [
        "If everything was done correctly, then `word_freq` should be a `Counter` object. Let's explore the data using this object. Answer the following questions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7vehcM5pTXy"
      },
      "source": [
        "# What are the 10 most common words used by Shakespeare and how often are they used?\n",
        "# (hint: The Counter's `most_common` function is useful)\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# How many unique words are there in Shakespeare's complete works?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# How many times does the word \"wherefore\" appear?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# How many times does the word \"romeo\" appear?\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "print(### YOUR ANSWER HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYvTApgFe-CH"
      },
      "source": [
        "So we lied to you. We won't actually be training our model for _every single word_ in the dataset. There are almost 30,000 unique words in the corpus, and finding the tallies for each word would take just too long, even for a computer! We're only going to look at the 1,000 most common words, which actually still gives pretty decent results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaG5dq4v4SSq"
      },
      "source": [
        "# Make a list containing the 1000 most common words in our corpus\n",
        "# We will use this list later\n",
        "# (hint: `most_common` will help here)\n",
        "\n",
        "### YOUR CODE HERE ### \n",
        "\n",
        "top_1000_words = ### YOUR ANSWER HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch this line!)\n",
        "assert 'whether' in top_1000_words and 'wert' in top_1000_words, \"`top_1000_words` seems to be incorrect!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IOv7AdpuDYN"
      },
      "source": [
        "# Finding Bigrams\n",
        "\n",
        "Now let's find all the bigrams in our corpus (regardless if they're the most common words or not). Iterate through the list `sentences` and create a new list of strings, where each string is a bigram:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8zHAiRMuQjj"
      },
      "source": [
        "# Create a list of bigrams by looping through the list `sentences`\n",
        "# The format of the list should be:\n",
        "# ['to be', 'be or', 'or not', 'not to', ...]\n",
        "\n",
        "bigrams = []\n",
        "for sentence in sentences:\n",
        "\n",
        "  # Make a list of words in the sentence\n",
        "  words_in_sentence = ### YOUR CODE HERE ###\n",
        "\n",
        "  # Loop through each word in the sentence (except the last)\n",
        "  for idx in range(len(words_in_sentence) - 1):\n",
        "\n",
        "    # Create the bigram from this word and the next\n",
        "    bigram = ### YOUR CODE HERE ###\n",
        "\n",
        "    bigrams.append(bigram)\n",
        "\n",
        "# you can print out the first couple entries to make sure you\n",
        "# have a sensible answer by running the following:\n",
        "print(bigrams[:10])\n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch this line!)\n",
        "assert 'motion seeming' in bigrams and 'forebetrayed and' in bigrams, \"You seem to be missing some bigrams!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Weqtv1jvDmG"
      },
      "source": [
        "# Training our Model\n",
        "\n",
        "Now let's make a dictionary that tallies the \"next word\" given a word. This will be the real workhorse of our algorithm.\n",
        "\n",
        "In detail, we will make a dictionary of dictionaries called `bigram_freqs` so that when we query `bigram_freqs[first_word][second_word]` we will get the number of times the `\"second_word\"` comes after `\"first_word\"` in our dataset.\n",
        "\n",
        "Again, we can't do this for every word in the dataset, so we'll only look at the 1000 most common words:\n",
        "\n",
        "Now fill out the blanks in the code below to generate our bigram frequency counter from `bigrams`, our list of bigrams. The code should take about 10 seconds to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sFgwU9KukuU"
      },
      "source": [
        "# Create bigram freqs matrix (don't worry about this code too much,\n",
        "# or feel free to ask about this)\n",
        "tally_factory = lambda: dict([(w, 1/1000) for w in top_1000_words])\n",
        "bigram_tallies = defaultdict(tally_factory)\n",
        "    \n",
        "# Fill up our bigram dictionary\n",
        "for bigram in tqdm(bigrams):    # tqdm gives us a progress bar\n",
        "\n",
        "    # Get the first and second word of our bigram\n",
        "    first_word, second_word = ### YOUR CODE HERE ###\n",
        "\n",
        "    # Check to make sure `first_word` and `second_word` are in the\n",
        "    # top 1000 most common words\n",
        "    if ### YOUR CODE HERE ###:\n",
        "\n",
        "        # Update the tallies\n",
        "        bigram_tallies[first_word][second_word] += ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "# Test to make sure you're on track (don't touch this line!)\n",
        "assert bigram_tallies['where']['art'] == 10.001, \"`bigram_tallies` seems to be incorrect!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNerO0rJ4w1d"
      },
      "source": [
        "Finally, let's write a function that predicts the next word given a word. Instead of choosing the next word exactly in proportion to the \"next word\" tallies, we'll do something slightly easier: Given a word, we will randomly choose one of the 20 most common \"next words\" to go next. Fill out the blanks in the next block to do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv1rcQTXxUlQ"
      },
      "source": [
        "def predict_next_word(word):\n",
        "    '''\n",
        "    Given `word`, find the top 10 most common words that\n",
        "    come next, and randomly return one of them\n",
        "    '''\n",
        "    # Only allow predictions on most common words\n",
        "    assert word in top_1000_words, \"Not a common word!\"\n",
        "    \n",
        "    # Get list of tallies for the word\n",
        "    # (hint: dictionaries have a function called `.values()`)\n",
        "    tallies = ### YOUR CODE HERE\n",
        "\n",
        "    # Get the index of the 20 most common words\n",
        "    # (this is done for you)\n",
        "    top_20_idxs = np.array(list(tallies)).argsort()[-20:]\n",
        "\n",
        "    # Get an index\n",
        "    # (hint: you need to select a random \n",
        "    # entry from the list `top_20_idxs`)\n",
        "    # (hint: try Googling)\n",
        "    rand_idx = ### YOUR CODE HERE ###\n",
        "    \n",
        "    # Find and return word the `rand_idx`th\n",
        "    # word from top_1000_words\n",
        "    next_word = ### YOUR CODE HERE ###\n",
        "    return next_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaWl4HpX6Z9C"
      },
      "source": [
        "Let's try out our function a few times on the word `\"his\"` a few times by re-running the next code block. Also try out other words by changing the argument of the function! For example, what words tend to come after `\"her\"`?\n",
        "\n",
        "You may notice that the words following `\"his\"` sometimes tend to be more masculine like `\"sword\"` or `\"majesty\"` while the words following `\"her\"` tend to be more feminine such as `\"love\"` or `\"husband\"`. This is because our prediction function is trained on Shakespeare's works, and as a result reflects his style of writing. Broadly, the idea that the data you train on affects your downstream model's predictions is called _**Dataset Bias**_. We will talk more about this later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24okpMjL9S-S"
      },
      "source": [
        "# Return a prediction of the next word\n",
        "print(predict_next_word('her'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7VFuPNO_PKv"
      },
      "source": [
        "Sometimes on a phone keyboard it's fun to just let autocomplete write entire sentences and see where it goes. We can do someting similar with our text prediction model. Let's start with the word `\"my\"` and predict the next word using our function, then feed that word _back into the next-word predictor_. Doing this a few times we can generate a sentence. \n",
        "\n",
        "Concretely, if we start with the word `\"when\"` and our algorithm predicts the word `\"is\"`, we should feed the word `\"is\"` _back into_ our predictor. Say we then get the word `\"he\"`. We now have the sentence `\"when is he...\"`. Continuing this process we can predict any number of words into the future.\n",
        "\n",
        "In computer science, we call this an _**autoregressive model**_ because it regresses to it's own output.\n",
        "\n",
        "Try to create a simple autoregressive model below using the function `predict_next_word` and run it a few times to see if it generates any sensible sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuXCfCij6Oe_"
      },
      "source": [
        "def autoregress(word):\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "autoregress('when')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM89Ugp3mBx1"
      },
      "source": [
        "# Dataset Bias\n",
        "\n",
        "Your autoregressive model should come up with some fairly decent sentences, and also some sentences that don't make any sense (this is a pretty simple model after all!). However, one thing you may notice is that all the sentences are very \"Shakespearian.\" This shouldn't be too big a surprise, because we trained our bigram model on Shakespeare! The formal term for this effect is called _**Dataset Bias**_, and we already saw an example of this earlier, when we looked at what sorts of words tended to follow the word `\"his\"` vs. `\"her\"`. Dataset bias is when the dataset we choose has an effect on our model's predictions.\n",
        "\n",
        "Let's try training a bigram model on a different dataset to see dataset bias in action. Run the following code to download, load, and clean a dataset of Amazon reviews on computer software products (this can take a bit of time, because the dataset is pretty large):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnCmK8yLnT5a"
      },
      "source": [
        "# Download amazon reviews from the interwebz and unzip it\n",
        "!wget -O amazon_software_reviews.json.gz http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Software.json.gz\n",
        "!gzip --force -d amazon_software_reviews.json.gz\n",
        "\n",
        "# Load the data into python\n",
        "with open('amazon_software_reviews.json', 'r+') as f:\n",
        "    reviews = f.readlines()\n",
        "    \n",
        "# Extract all the text and clean\n",
        "reviews = [json.loads(review).get('reviewText', '') for review in reviews]\n",
        "reviews = reviews[:int(len(reviews) * .1)]  # Only use 10% of dataset\n",
        "amazon_text = ' '.join(reviews)\n",
        "amazon_text = amazon_text.lower()\n",
        "pattern = re.compile('[^a-zA-Z_ .!?]+')\n",
        "amazon_text = pattern.sub('', amazon_text)\n",
        "amazon_text = re.sub(\"\\s+\",\" \", amazon_text)\n",
        "sentences = re.split('[.!?]', amazon_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kiFEib-p_WB"
      },
      "source": [
        "Now all of the Amazon review sentences are in the variable `sentences` (we've overwritten our Shakespeare sentences). This means you can go back to the section called **The Bigram Model** and rerun all the blocks below to train a bigram model on these Amazon reviews. Go ahead and do that, and see what sorts of sentences you get.\n",
        "\n",
        "You should have gotten sentences that look something like\n",
        "\n",
        "- `\"when youre doing some very fast pc running a better off\"`\n",
        "- `\"when we do some serious computer i am extremely happy and\"`\n",
        "- `\"when using windows in my new interface in that the computer\"`\n",
        "\n",
        "these sentences are very different from the Shakespeare sentences, and this is because we're using a different dataset. \n",
        "\n",
        "Sometimes dataset bias can manifest itself in pretty obvious ways, like what we've just seen. But other times, dataset bias can be problematic. If the dataset we're using contains bad language, or racism, or sexism, then our model might learn these tendencies. For this reason, we have to be really careful when choosing and cleaning our data. For more information, check [this article](https://www.vox.com/videos/2021/3/31/22348722/ai-bias-racial-machine-learning) out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTsD7rUUuzM9"
      },
      "source": [
        "# Modern Language Models\n",
        "\n",
        "The bigram model that we've been creating was first developed in the 1950's by Claude Shannon (a University of Michigan alumni!). Because these are such old and basic models, you might have noticed that a lot of the time the sentences just didn't make any sense. \n",
        "\n",
        "NLP has come a long way since the 50's. Nowadays, the best language models are class of models called _**transformers**_. To give you an idea of how powerful these models are, let's play around with some!\n",
        "\n",
        "We will use a model called [GPT-2](https://openai.com/blog/better-language-models/), released by OpenAI, a company that specializes in AI research. GPT-2 is one of the best models today, and is trained on data taken from Wikipedia, Reddit, and other places on the internet. Run the code below to download all the necessary components (this may take around a minute):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCV-CPueFWQY"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "# import relevant packages\n",
        "from transformers import pipeline\n",
        "\n",
        "# create a pipline and specify the gpt2 model for text generation\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua-xH_oGxQ8M"
      },
      "source": [
        "After running the above block, we should have downloaded GPT-2 and set it up for text prediction. To play around with it, check out the next block of code. \n",
        "\n",
        "Interesting things to think about include:\n",
        "\n",
        "- Is the model simply memorizing sentences from the internet? One way you can test this is Googling the output of the model to see if people have written it before.\n",
        "- Can you get the model to mess up with crazy inputs? If so what inputs did you use?\n",
        "- Try changing the `max_length` parameter to output longer predictions. Does the output of the model make any sense? Is it consistent over long predictions?\n",
        "- Can the model answer questions? Try giving the model questions (such as `\"What color is the sky?\"`) as input.\n",
        "- Does the model tend to have any biases? Does it tend to talk about a particular topic excessively?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm6kavFmuMii"
      },
      "source": [
        "# Given the partial sentence, GPT-2 will try to continue it\n",
        "# Try changing input sentence and rerunning this cell to see what GPT-2 outputs\n",
        "partial_sentence = \"When in the course of human events\"\n",
        "generator(partial_sentence, max_length=30, num_return_sequences=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpI2do2q2UCZ"
      },
      "source": [
        "# The End\n",
        "\n",
        "Hopefully now you have some understanding of what NLP is! Just to recap, we've:\n",
        "\n",
        "- Explored large datasets of text\n",
        "- Learned how to clean these large datasets of text\n",
        "- Built an autocomplete function\n",
        "- Investigated the effects of data bias \n",
        "- Played around with modern language models\n",
        "\n",
        "If you're still curious you can try to complete another notebook, or ask the instructors for more things to do. Other things to try in this notebook include:\n",
        "\n",
        "- Trying out other text datasets \n",
        "- Cleaning the Shakespeare data better\n",
        "- Try to find other sources of bias in the Shakespeare dataset"
      ]
    }
  ]
}